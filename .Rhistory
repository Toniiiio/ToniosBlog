links %<>%
{ifelse(
test = not_matched,
yes = paste0("https://www.", domain, "/", .),
no = .)
}
return(links)
}
links %<>% filter_links(parsed_links = parsed_links)
has_new_links <- length(links)
while(has_new_links){
iter_nr <- iter_nr + 1
# catch document and all links
all_links <- list()
all_docs <- list()
for(link in links){
print(link)
all_docs[[link]] <- tryCatch(
expr = link %>% httr::GET() %>% httr::content(encoding = "UTF-8"),
error = function(e) ""
)
has_doc <- nchar(all_docs[[link]])
if(has_doc){
all_links[[link]] <- all_docs[[link]] %>%
html_nodes(xpath = "//a") %>%
html_attr(name = "href")  %<>%
{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", domain, .), no = .)}
}else{
all_links[[link]] <- ""
}
}
links_per_level[[iter_nr]] <- all_links
docs_per_level[[iter_nr]] <- all_docs
all_links %<>% unlist %>% unname
links <- all_links %>% filter_links(parsed_links = parsed_links)
has_new_links <- length(links)
parsed_links <- c(parsed_links, links)
}
all_docs <- docs_per_level %>% unlist(recursive = FALSE)
ss <- lapply(all_docs, function(doc){
if(!nchar(doc)) return(FALSE)
doc %>%
html_text %>%
tolower %>%
gsub(pattern = "\n|\t|\r", replacement = "") %>%
aregexec(pattern = tolower(target), max.distance = 0.1) %>%
unlist %>% "!="(-1) %>% sum
})
ss %>% unlist %>% ">"(0) %>% which
target_url <- "https://www.dzbank.de/content/dzbank_de/de/home/unser_profil/karriere/berufserfahrene.html"
url <- "https://www.dzbank.de/content/dzbank_de/de/home.html"
target <- "Senior Revisor IT"
tolower(target) == target2
txt <- target_url %>%
read_html %>% html_text %>%
tolower %>%
gsub(pattern = "\n|\t", replacement = " ")
txt %>% mypkg2:::show_html_page()
has_match <- txt %>% aregexec(pattern = tolower(target), max.distance = 0.1) %>%
unlist %>% "!="(-1) %>% sum
has_match
# grab all links from that url
domain <- urltools::domain(url) %>%
gsub(pattern = "www.", replacement = "")
parsed_links <- c()
links_per_level <- list()
docs_per_level <- list()
doc <- url %>% read_html
links <- doc %>%
html_nodes(xpath = "//a") %>%
html_attr(name = "href")
iter_nr <- 1
links_per_level[1] <- url
# https
# fileadmin
# javascript
# mailto:
# file:///
filter_links <- function(links, parsed_links){
links %<>%
{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", domain, .), no = .)}
domains <- links %>%
urltools::domain() %>%
gsub(pattern = "www.", replacement = "")
same_domain <- is.na(domains) | domains == domain
hash_link <- substr(links, 1, 1) == "#"
is_empty <- !nchar(links)
is_na <- is.na(links)
alr_exist <- links %in% parsed_links
is_html <- sapply(c("mailto:", ".tif", ".mp4", ".mp3", ".tiff", ".png", ".gif", ".jpeg",".jpg", ".zip", ".pdf"),
FUN = grepl, x = links) %>%
rowSums %>%
magrittr::not()
links %<>%
.[!hash_link & !is_na & !is_empty & !alr_exist & same_domain & is_html] %>%
unique
not_matched <- grepl(substring(text = links, first = 1, last = 1), pattern = "[a-z]", perl = TRUE) &
!grepl(links, pattern = "https://") &
!grepl(links, pattern = "www.")
links %<>%
{ifelse(
test = not_matched,
yes = paste0("https://www.", domain, "/", .),
no = .)
}
return(links)
}
links %<>% filter_links(parsed_links = parsed_links)
has_new_links <- length(links)
while(has_new_links){
iter_nr <- iter_nr + 1
# catch document and all links
all_links <- list()
all_docs <- list()
for(link in links){
print(link)
all_docs[[link]] <- tryCatch(
expr = link %>% httr::GET() %>% httr::content(encoding = "UTF-8"),
error = function(e) ""
)
has_doc <- nchar(all_docs[[link]])
if(has_doc){
all_links[[link]] <- all_docs[[link]] %>%
html_nodes(xpath = "//a") %>%
html_attr(name = "href")  %<>%
{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", domain, .), no = .)}
}else{
all_links[[link]] <- ""
}
}
links_per_level[[iter_nr]] <- all_links
docs_per_level[[iter_nr]] <- all_docs
all_links %<>% unlist %>% unname
links <- all_links %>% filter_links(parsed_links = parsed_links)
has_new_links <- length(links)
parsed_links <- c(parsed_links, links)
}
all_docs <- docs_per_level %>% unlist(recursive = FALSE)
ss <- lapply(all_docs, function(doc){
if(!nchar(doc)) return(FALSE)
doc %>%
html_text %>%
tolower %>%
gsub(pattern = "\n|\t|\r", replacement = "") %>%
aregexec(pattern = tolower(target), max.distance = 0.1) %>%
unlist %>% "!="(-1) %>% sum
})
ss %>% unlist %>% ">"(0) %>% which
has_doc
link
all_docs[[link]]
all_docs[[link]] %>% toString
has_doc <- nchar(all_docs[[link]] %>% toString)
has_doc
while(has_new_links){
iter_nr <- iter_nr + 1
# catch document and all links
all_links <- list()
all_docs <- list()
for(link in links){
print(link)
all_docs[[link]] <- tryCatch(
expr = link %>% httr::GET() %>% httr::content(encoding = "UTF-8"),
error = function(e) ""
)
has_doc <- nchar(all_docs[[link]] %>% toString)
if(has_doc){
all_links[[link]] <- all_docs[[link]] %>%
html_nodes(xpath = "//a") %>%
html_attr(name = "href")  %<>%
{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", domain, .), no = .)}
}else{
all_links[[link]] <- ""
}
}
links_per_level[[iter_nr]] <- all_links
docs_per_level[[iter_nr]] <- all_docs
all_links %<>% unlist %>% unname
links <- all_links %>% filter_links(parsed_links = parsed_links)
has_new_links <- length(links)
parsed_links <- c(parsed_links, links)
}
all_docs <- docs_per_level %>% unlist(recursive = FALSE)
ss <- lapply(all_docs, function(doc){
if(!nchar(doc)) return(FALSE)
doc %>%
html_text %>%
tolower %>%
gsub(pattern = "\n|\t|\r", replacement = "") %>%
aregexec(pattern = tolower(target), max.distance = 0.1) %>%
unlist %>% "!="(-1) %>% sum
})
ss %>% unlist %>% ">"(0) %>% which
link
all_docs[[link]]
has_doc <- nchar(all_docs[[link]] %>% toString)
if(has_doc){
all_links[[link]] <- all_docs[[link]] %>%
html_nodes(xpath = "//a") %>%
html_attr(name = "href")  %<>%
{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", domain, .), no = .)}
}else{
all_links[[link]] <- ""
}
links_per_level[[iter_nr]] <- all_links
docs_per_level[[iter_nr]] <- all_docs
all_links %<>% unlist %>% unname
links <- all_links %>% filter_links(parsed_links = parsed_links)
has_new_links <- length(links)
parsed_links <- c(parsed_links, links)
links
link
has_doc
library(rvest)
library(httr)
library(magrittr)
# open issues
# could be in pdf -
# than go over domain https://recalm.com/wp-content/uploads/2019/07/D2019_07_03_Ausschreibung_Abschlussarbeit_Elektrotechnik.pdf
# start in der anderen domäne
# https://recruitingapp-5118.de.umantis.com/Vacancies/2972/Description/1?lang=ger&DesignID=00
# start with a url
# url <- "https://www.fida.de/jobs/stellenangebote/"
# target <- "werkstudent bereich personal"
# url <- "https://careers.cid.com/de"
# target <- "Informatiker (m/w)"
# url <- "https://recalm.com/"
# target <- "Praktikum Entrepreneurship (m/w/d)"
url <- "https://www.deutsche-leasing.com/de"
target_url <- "https://www.deutsche-leasing.com/de/unternehmen/karriere/unsere-stellenangebote"
target <- "Strategischer IT-Einkäufer (m/w/d)"
target_url <- "https://www.dzbank.de/content/dzbank_de/de/home/unser_profil/karriere/berufserfahrene.html"
url <- "https://www.dzbank.de/content/dzbank_de/de/home.html"
target <- "Senior Revisor IT"
target <- "DATA INTEGRATION ENGINEER (M/W/D)"
target_url <- "https://veact-jobs.personio.de/"
url <- "https://veact-jobs.personio.de/job/80470"
tolower(target) == target2
txt <- target_url %>%
read_html %>% html_text %>%
tolower %>%
gsub(pattern = "\n|\t", replacement = " ")
txt %>% mypkg2:::show_html_page()
has_match <- txt %>% aregexec(pattern = tolower(target), max.distance = 0.1) %>%
unlist %>% "!="(-1) %>% sum
has_match
# grab all links from that url
domain <- urltools::domain(url) %>%
gsub(pattern = "www.", replacement = "")
parsed_links <- c()
links_per_level <- list()
docs_per_level <- list()
doc <- url %>% read_html
links <- doc %>%
html_nodes(xpath = "//a") %>%
html_attr(name = "href")
iter_nr <- 1
links_per_level[1] <- url
# filter links
# exclude non .html, e.g. .jpgs, .zip
# https
# fileadmin
# javascript
# mailto:
# file:///
filter_links <- function(links, parsed_links){
links %<>%
{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", domain, .), no = .)}
domains <- links %>%
urltools::domain() %>%
gsub(pattern = "www.", replacement = "")
same_domain <- is.na(domains) | domains == domain
hash_link <- substr(links, 1, 1) == "#"
is_empty <- !nchar(links)
is_na <- is.na(links)
alr_exist <- links %in% parsed_links
is_html <- sapply(c("mailto:", ".tif", ".mp4", ".mp3", ".tiff", ".png", ".gif", ".jpeg",".jpg", ".zip", ".pdf"),
FUN = grepl, x = links) %>%
rowSums %>%
magrittr::not()
links %<>%
.[!hash_link & !is_na & !is_empty & !alr_exist & same_domain & is_html] %>%
unique
not_matched <- grepl(substring(text = links, first = 1, last = 1), pattern = "[a-z]", perl = TRUE) &
!grepl(links, pattern = "https://") &
!grepl(links, pattern = "www.")
links %<>%
{ifelse(
test = not_matched,
yes = paste0("https://www.", domain, "/", .),
no = .)
}
return(links)
}
links %<>% filter_links(parsed_links = parsed_links)
has_new_links <- length(links)
while(has_new_links){
iter_nr <- iter_nr + 1
# catch document and all links
all_links <- list()
all_docs <- list()
for(link in links){
print(link)
all_docs[[link]] <- tryCatch(
expr = link %>% httr::GET() %>% httr::content(encoding = "UTF-8"),
error = function(e) ""
)
has_doc <- nchar(all_docs[[link]] %>% toString)
if(has_doc){
all_links[[link]] <- all_docs[[link]] %>%
html_nodes(xpath = "//a") %>%
html_attr(name = "href")  %<>%
{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", domain, .), no = .)}
}else{
all_links[[link]] <- ""
}
}
links_per_level[[iter_nr]] <- all_links
docs_per_level[[iter_nr]] <- all_docs
all_links %<>% unlist %>% unname
links <- all_links %>% filter_links(parsed_links = parsed_links)
has_new_links <- length(links)
parsed_links <- c(parsed_links, links)
}
all_docs <- docs_per_level %>% unlist(recursive = FALSE)
ss <- lapply(all_docs, function(doc){
if(!nchar(doc)) return(FALSE)
doc %>%
html_text %>%
tolower %>%
gsub(pattern = "\n|\t|\r", replacement = "") %>%
aregexec(pattern = tolower(target), max.distance = 0.1) %>%
unlist %>% "!="(-1) %>% sum
})
ss %>% unlist %>% ">"(0) %>% which
# rr[url]
# doc <- url %>% read_html
# doc %>% mypkg2:::show_html_page()
# names(rr) == url
#
# rr[[2]] %>% html_text()
15000/411
library(rvest)
library(httr)
library(magrittr)
# open issues
# could be in pdf -
# than go over domain https://recalm.com/wp-content/uploads/2019/07/D2019_07_03_Ausschreibung_Abschlussarbeit_Elektrotechnik.pdf
# start in der anderen domäne
# https://recruitingapp-5118.de.umantis.com/Vacancies/2972/Description/1?lang=ger&DesignID=00
# start with a url
# url <- "https://www.fida.de/jobs/stellenangebote/"
# target <- "werkstudent bereich personal"
# url <- "https://careers.cid.com/de"
# target <- "Informatiker (m/w)"
# url <- "https://recalm.com/"
# target <- "Praktikum Entrepreneurship (m/w/d)"
url <- "https://www.deutsche-leasing.com/de"
target_url <- "https://www.deutsche-leasing.com/de/unternehmen/karriere/unsere-stellenangebote"
target <- "Strategischer IT-Einkäufer (m/w/d)"
target_url <- "https://www.dzbank.de/content/dzbank_de/de/home/unser_profil/karriere/berufserfahrene.html"
url <- "https://www.dzbank.de/content/dzbank_de/de/home.html"
target <- "Senior Revisor IT"
target <- "DATA INTEGRATION ENGINEER (M/W/D)"
target_url <- "https://veact-jobs.personio.de/"
url <- "https://veact-jobs.personio.de/job/80470"
tolower(target) == target2
txt <- target_url %>%
read_html %>% html_text %>%
tolower %>%
gsub(pattern = "\n|\t", replacement = " ")
txt %>% mypkg2:::show_html_page()
has_match <- txt %>% aregexec(pattern = tolower(target), max.distance = 0.1) %>%
unlist %>% "!="(-1) %>% sum
has_match
# grab all links from that url
domain <- urltools::domain(url) %>%
gsub(pattern = "www.", replacement = "")
parsed_links <- c()
links_per_level <- list()
docs_per_level <- list()
doc <- url %>% read_html
links <- doc %>%
html_nodes(xpath = "//a") %>%
html_attr(name = "href")
iter_nr <- 1
links_per_level[1] <- url
# filter links
# exclude non .html, e.g. .jpgs, .zip
# https
# fileadmin
# javascript
# mailto:
# file:///
filter_links <- function(links, parsed_links){
links %<>%
{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", domain, .), no = .)}
domains <- links %>%
urltools::domain() %>%
gsub(pattern = "www.", replacement = "")
same_domain <- is.na(domains) | domains == domain
hash_link <- substr(links, 1, 1) == "#"
is_empty <- !nchar(links)
is_na <- is.na(links)
alr_exist <- links %in% parsed_links
is_html <- sapply(c("mailto:", ".tif", ".mp4", ".mp3", ".tiff", ".png", ".gif", ".jpeg",".jpg", ".zip", ".pdf"),
FUN = grepl, x = links) %>%
rowSums %>%
magrittr::not()
links %<>%
.[!hash_link & !is_na & !is_empty & !alr_exist & same_domain & is_html] %>%
unique
not_matched <- grepl(substring(text = links, first = 1, last = 1), pattern = "[a-z]", perl = TRUE) &
!grepl(links, pattern = "https://") &
!grepl(links, pattern = "www.")
links %<>%
{ifelse(
test = not_matched,
yes = paste0("https://www.", domain, "/", .),
no = .)
}
return(links)
}
links %<>% filter_links(parsed_links = parsed_links)
has_new_links <- length(links)
while(has_new_links){
iter_nr <- iter_nr + 1
# catch document and all links
all_links <- list()
all_docs <- list()
for(link in links){
print(link)
all_docs[[link]] <- tryCatch(
expr = link %>% httr::GET() %>% httr::content(encoding = "UTF-8"),
error = function(e) ""
)
has_doc <- nchar(all_docs[[link]] %>% toString)
if(has_doc){
all_links[[link]] <- all_docs[[link]] %>%
html_nodes(xpath = "//a") %>%
html_attr(name = "href")  %<>%
{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", domain, .), no = .)}
}else{
all_links[[link]] <- ""
}
}
links_per_level[[iter_nr]] <- all_links
docs_per_level[[iter_nr]] <- all_docs
all_links %<>% unlist %>% unname
links <- all_links %>% filter_links(parsed_links = parsed_links)
has_new_links <- length(links)
parsed_links <- c(parsed_links, links)
}
all_docs <- docs_per_level %>% unlist(recursive = FALSE)
ss <- lapply(all_docs, function(doc){
if(!nchar(doc)) return(FALSE)
doc %>%
html_text %>%
tolower %>%
gsub(pattern = "\n|\t|\r", replacement = "") %>%
aregexec(pattern = tolower(target), max.distance = 0.1) %>%
unlist %>% "!="(-1) %>% {sum(.) / length(.)}
})
ss %>% unlist %>% ">"(0.3) %>% which
# rr[url]
# doc <- url %>% read_html
# doc %>% mypkg2:::show_html_page()
# names(rr) == url
#
# rr[[2]] %>% html_text()
res <- httr::GET("https://www.bing.com/search?q=quentic+jobs")
res
content(res)
content(res) %>% mypkg2:::show_html_page()
res <- httr::GET("https://www.bing.com/search?q=quentic+jobs") %>% httr::content()
res <- httr::GET("https://www.bing.com/search?q=quentic+jobs") %>% httr::content()
doc <- html_nodes(xpath = "/html/body/div[2]/main/ol/li[3]/h2/a")
doc <- html_nodes(doc, xpath = "/html/body/div[2]/main/ol/li[3]/h2/a")
doc <- httr::GET("https://www.bing.com/search?q=quentic+jobs") %>% httr::content()
nodes <- html_nodes(doc, xpath = "/html/body/div[2]/main/ol/li[3]/h2/a")
nodes
res %>% mypkg2:::show_html_page()
133/6.75
load("contents.RData")
blogdown::build_site()
getwd()
setwd("blog6")
blogdown::serve_site()
blogdown::build_site()
setwd("blog6")
getwd()
#blogdown::new_site(theme = "shenoybr/hugo-goa")
blogdown::serve_site()
#blogdown::new_site(theme = "shenoybr/hugo-goa")
blogdown::serve_site()
blogdown::stop_server()
#blogdown::new_site(theme = "shenoybr/hugo-goa")
blogdown::serve_site()
getwd()
setwd("../blog5")
#blogdown::new_site(theme = "shenoybr/hugo-goa")
blogdown::serve_site()
#blogdown::new_site(theme = "shenoybr/hugo-goa")
blogdown::serve_site()
blogdown:::new_post_addin()
blogdown:::build_site()
blogdown:::build_site()
blogdown::build_site()
#blogdown::new_site(theme = "shenoybr/hugo-goa")
blogdown::serve_site()
load("contents.RData")
getwd()
