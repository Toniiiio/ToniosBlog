---
title: Sneak preview sivis - Turning browser clicks intro reproducible scraping code
author: Tonio Liebrand
date: '2020-07-10'
slug: sneak-preview-sivis-turning-browser-clicks-intro-reproducible-scraping-code
categories: []
tags:
  - R
  - sivis
  - scraping
comments: no
showcomments: no
showpagemeta: yes
---

Sivis enables you to select data in the browser by clicking on it and then generates r code for you to scrape this data periodically. For a preview see the following video.

[![Sivis example](/img/youtube.png)](https://www.youtube.com/watch?v=tFZ3os-GoNA)

I am working on this package for over a year now. Maybe its too early to leave the pre-alpha state, but it guess there is a certain time to put it out there: [Github repo](https://github.com/Toniiiio/sivis){target="_blank"}.


## How does sivis work?

Scraping data can be summarized in three steps:

1. Identify a request that will return us a document containing the data we attempt to scrape

2. Find a path to extract the data from the document

3. Schedule, monitor and maintain the scraper

Sivis well assist you building and monitoring scrapers by automising repetitive steps. 

This blog post introduces you to the basics of sivis.


### 1. Request identification

#### The idea 

When you learn about webscraping you are usually taught to differentiate between javascript and non-javascript pages and use httr/rvest or (r)selenium. You would either send a single request (httr/rvest) or trigger all requests (RSelenium) related to a web page.
I think this differentiation is not wrong, but i would propose an alternative view. 

> A website consists on average of 70 requests, only one will yield your target information. Why would you want to load the other 69?

One might argue that it is not easy to identify the correct request among the ~70 candidates and tideous work to collect the relevant meta data for building GET / POST requests. That is correct, if it is attempted to be done manually. But sivis will assist you to automize this process as much as possible.

#### In a nutshell
The identification of the correct request will be performed in a chrome extension. When the user opens a webpage in chrome, the browser will automatically perform all required requests related to this webpage. My idea was to interpose this process and check the contents of all server responses for our target data. Moreover, we can save the meta data being used for the requests, like the request header and -body.
To select the target data the user will be given the possibility to select texts within the browser.

Aggregating this information will yield us a rich data set, which will be sufficient to build a scraper from within R.
This dataset will be transformed to a JSON object and loaded into the clipboard from where it can be accessed in R.

### 2. Path extraction

In order to find a path to extract the data from a given document, we first have to load the server response in R by building and executing a valid request. We have the data that yielded a successful request from the browser. This data can not be used one to one, but is a great starting point and will yield succesful requests on most web pages that do not prohibit web scraping.

Given this request we can download the document containg our target information. In order to find a path we first have to know which kind of document is present. It will be mostly either of type html/xml, a JSON or javascript code containing a JSON.*

In case of an html/xml document an xpath will be generated to extract the target information. For JSONS the extraction mechanisms aren´t as powerful as xpath, but a similar path will be generated.

At the heart of this step are the function get_xpath_by_tag or get_xpath_by_text. Given an xml/html document the node(s) containing the target data will be identified and then the xpath will be generated "going up the tree" without adding unrelevant data or leaving relevant data behind.

### 3. Setting in production

Webpage structures change over time: The relevant request, the related request header or -body, as well as pathes to extract the target data might change. The scrapers have to be updated accordingly.
To identify failed scrapers a logging framework is required. As the source of errors for scrapers is limited and their occurence highly repetitive, a logging and monitoring framework will be built specific to the needs of web scraping.

## Deep dive

For more information feel free to contact me directly or check the growing documentation on github.

*It might rarely be the case that the target data is stored in unstructured text or the structure is intentionally modified that the document can´t be identified as a JSON or html.
